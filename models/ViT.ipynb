{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import vit_b_16, vit_l_16, vit_b_32, vit_l_32\n",
    "from torchvision.transforms import Resize, Compose, ToTensor\n",
    "from tqdm import tqdm\n",
    "from datasets import ClassLabel\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "from data import load_food\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam import GradCAM\n",
    "\n",
    "from config import OUTPUT_DIR\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# importing data\n",
    "train_ds, val_ds = load_food.load_food()"
   ],
   "id": "3b57d75383b5da23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# constants\n",
    "NUM_CLASS = 20\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "# creating label to id and id to label mappings\n",
    "labels = train_ds.features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "    \n",
    "# selected classes\n",
    "selected_classes = ['ramen', 'carrot_cake', 'beef_carpaccio', 'strawberry_shortcake', 'escargots', 'donuts', 'croque_madame', 'cheese_plate', 'caprese_salad', 'sashimi', 'oysters', 'caesar_salad', 'pho', 'hot_and_sour_soup', 'beef_tartare', 'creme_brulee', 'cup_cakes', 'miso_soup', 'pork_chop', 'paella']\n",
    "\n",
    "# remapping to new indices\n",
    "def map_to_new_label(image, old_mapping, new_class_names):\n",
    "    old_name = old_mapping[str(image[\"label\"])]\n",
    "    new_id = new_class_names.index(old_name)\n",
    "    image[\"label\"] = new_id\n",
    "    return image\n",
    "\n",
    "def transform_fn(batch):\n",
    "    batch[\"image\"] = [map_to_new_label(img, old_mapping=id2label, new_class_names=selected_classes) for img in batch[\"image\"]]\n",
    "    return batch\n",
    "\n",
    "train_ds = train_ds.with_transform(transform_fn)\n",
    "val_ds = val_ds.with_transform(transform_fn)\n",
    "new_label_feature = ClassLabel(names=selected_classes)\n",
    "train_ds = train_ds.cast_column(\"label\", new_label_feature)\n",
    "val_ds = val_ds.cast_column(\"label\", new_label_feature)\n",
    "\n",
    "# image transformation\n",
    "train_transforms = Compose([\n",
    "    Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "val_transforms = Compose([\n",
    "    Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "def map_fn(examples):\n",
    "    examples[\"pixel_values\"] = [train_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples\n",
    "\n",
    "train_ds_transformed = train_ds.with_transform(map_fn)\n",
    "val_ds_transformed = val_ds.with_transform(map_fn)"
   ],
   "id": "147b962dacb319d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# configuring the device used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "\n",
    "def get_vit_architecture(architecture_name):\n",
    "  if architecture_name == \"vit_b_16\":\n",
    "    return vit_b_16\n",
    "  elif architecture_name == \"vit_l_16\":\n",
    "    return vit_l_16\n",
    "  elif architecture_name == \"vit_b_32\":\n",
    "    return vit_b_32\n",
    "  elif architecture_name == \"vit_l_32\":\n",
    "    return vit_l_32\n",
    "  else:\n",
    "    raise ValueError(f\"Unknown architecture name: {architecture_name}\")\n",
    "\n",
    "# the vision transformer model\n",
    "def get_custom_vit(custom_head: nn.Sequential, architecture_name: str, weights=None, device=device):\n",
    "  print(f\"\\nArchitecture: {architecture_name}; Weights: {weights}\\n\")\n",
    "  architecture = get_vit_architecture(architecture_name)\n",
    "  if weights is None:\n",
    "    vit_model = architecture()\n",
    "  else:\n",
    "    vit_model = architecture(weights=weights)\n",
    "  vit_model.heads.head = custom_head\n",
    "  model = vit_model\n",
    "  model.to(device)\n",
    "\n",
    "  for param in vit_model.parameters():\n",
    "      param.requires_grad = False\n",
    "  for param in vit_model.heads.head.parameters():\n",
    "      param.requires_grad = True\n",
    "\n",
    "  return model"
   ],
   "id": "d3ed291549744907"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# running and validating the model\n",
    "def run_model(model, train_loader, val_loader, hyperparameters, device=device):\n",
    "  optimizer = hyperparameters['optimizer'](model.parameters(), lr=hyperparameters['lr'])\n",
    "  loss_fct = CrossEntropyLoss()\n",
    "  history = []\n",
    "  n_epochs = hyperparameters['n_epochs']\n",
    "  for epoch in range(n_epochs):\n",
    "    history_item = {}\n",
    "    history_item['epoch'] = epoch\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    train_correct, train_total = 0, 0\n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} in training\"):\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      train_x = batch[\"pixel_values\"]\n",
    "      train_y = batch[\"label\"]\n",
    "      train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "      train_y_hat = model(train_x)\n",
    "      loss = loss_fct(train_y_hat, train_y)\n",
    "\n",
    "      train_loss += loss.item()/ len(train_loader)\n",
    "      train_correct += torch.sum(torch.argmax(train_y_hat, dim=1) == train_y).item()\n",
    "      train_total += len(train_x)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{n_epochs} loss: {train_loss:.3f}\")\n",
    "    print(f\"Training accuracy: {train_correct / train_total * 100:.2f}%\")\n",
    "    history_item['train_loss'] = train_loss\n",
    "    history_item['train_accuracy'] = train_correct / train_total * 100\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      val_correct, val_total = 0, 0\n",
    "      val_loss = 0.0\n",
    "      for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "          val_x = batch[\"pixel_values\"]\n",
    "          val_y = batch[\"label\"]\n",
    "          val_x, val_y = val_x.to(device), val_y.to(device)\n",
    "          val_y_hat = model(val_x)\n",
    "          loss = loss_fct(val_y_hat, val_y)\n",
    "          val_loss += loss.item()/ len(val_loader)\n",
    "\n",
    "          val_correct += torch.sum(torch.argmax(val_y_hat, dim=1) == val_y).item()\n",
    "          val_total += len(val_x)\n",
    "      print(f\"Validation loss: {val_loss:.3f}\")\n",
    "      print(f\"Validation accuracy: {val_correct / val_total * 100:.2f}%\")\n",
    "      history_item['val_loss'] = val_loss\n",
    "      history_item['val_accuracy'] = val_correct / val_total * 100\n",
    "      history.append(history_item)\n",
    "\n",
    "  return model, history"
   ],
   "id": "2ed703544d0dba28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_predictions(model, dataloader, device):\n",
    "    print('\\nGetting predictions...\\n')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_val_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch_x = batch[\"pixel_values\"]\n",
    "            batch_y = batch[\"label\"]\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            batch_y_hat = model(batch_x)\n",
    "            loss = CrossEntropyLoss()(batch_y_hat, batch_y)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(batch_y_hat, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(dataloader)\n",
    "    return all_preds, all_labels, avg_val_loss, total_val_loss\n",
    "\n",
    "def get_metrics(all_preds, all_labels):\n",
    "    print(f'\\nCalculating metrics...')\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Accuracy:              {accuracy:.4f}\")\n",
    "    print(f\"Precision (macro):     {precision:.4f}\")\n",
    "    print(f\"Recall (macro):        {recall:.4f}\")\n",
    "    print(f\"F1-score (macro):      {f1:.4f}\\n\")\n",
    "\n",
    "def plot_confusion_matrix(all_labels, all_preds, class_names=id2label.values()):\n",
    "    print(f'\\nPlotting confusion matrix...')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    np.fill_diagonal(cm, 0)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "        vmax=21\n",
    "    )\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Vision Transformer\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def run_evaluation(model, dataloader, metrics=True, confusion_matrix=False, device=device):\n",
    "    all_preds, all_labels, avg_val_loss, total_val_loss = get_predictions(model, dataloader, device)\n",
    "    print(f'\\nRunning evaluation...\\n')\n",
    "\n",
    "    # Compute metrics\n",
    "    if metrics:\n",
    "      get_metrics(all_preds, all_labels)\n",
    "\n",
    "    # Plotting confusion matrix\n",
    "    if confusion_matrix:\n",
    "      plot_confusion_matrix(all_labels, all_preds, class_names=id2label)"
   ],
   "id": "a610bec111ba2335"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def save_model_and_history(experiment_name, model, history):\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    output_pth = os.path.join(OUTPUT_DIR, f'experiments/torch_model_{experiment_name}.pth')\n",
    "    torch.save(model.state_dict(), output_pth)\n",
    "    print(f\"Saved model to: {output_pth}\")\n",
    "    \n",
    "    output_json = os.path.join(OUTPUT_DIR, f\"experiments/training_history_{experiment_name}.json\")\n",
    "    with open(output_json, \"w\") as f:\n",
    "      json.dump(history, f, indent=2)\n",
    "    print(f\"Saved history to: {output_json}\")"
   ],
   "id": "ad5e4502569144f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "hyperparameters = {\n",
    "    'batch_size': 16,\n",
    "    'n_epochs': 10,\n",
    "    'lr': 0.001,\n",
    "    'optimizer': AdamW\n",
    "}\n",
    "\n",
    "train_loader = DataLoader(train_ds_transformed, shuffle=True, batch_size=hyperparameters['batch_size'])\n",
    "val_loader = DataLoader(val_ds_transformed, shuffle=False, batch_size=hyperparameters['batch_size'])"
   ],
   "id": "21ad5bb37bc86cf3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "top_layers = nn.Sequential(\n",
    "    nn.Linear(768, NUM_CLASS)\n",
    "  )\n",
    "model = get_custom_vit(custom_head=top_layers, architecture_name=\"vit_b_16\", weights='IMAGENET1K_V1')\n",
    "model, history = run_model(model, train_loader, val_loader, hyperparameters)\n",
    "save_model_and_history(\"vit_base_16_cls_epoch10\", model, history)"
   ],
   "id": "fb45eab3a871a0de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "top_layers = nn.Sequential(\n",
    "    nn.Linear(768, NUM_CLASS)\n",
    ")\n",
    "model2 = get_custom_vit(custom_head=top_layers, architecture_name=\"vit_b_16\", weights=None)\n",
    "model2.load_state_dict(torch.load(OUTPUT_DIR + '/experiments/torch_model_vit_base_16_cls_epoch10.pth'))\n",
    "run_evaluation(model2, val_loader)\n",
    "all_labels, all_preds, _, _ = get_predictions(model2, val_loader, device)\n",
    "plot_confusion_matrix(all_labels, all_preds)"
   ],
   "id": "16c4840c22f71dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plot_confusion_matrix(all_labels, all_preds)",
   "id": "579917785a0d7160"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from config import GRAD_CAM_DIR\n",
    "\n",
    "def reshape_transform(tensor, height=14, width=14):\n",
    "    if isinstance(tensor, tuple):\n",
    "        tensor = tensor[0]\n",
    "    tensor = tensor[:, 1:, :]\n",
    "    B, _, D = tensor.shape\n",
    "    tensor = tensor.reshape(B, height, width, D).permute(0, 3, 1, 2)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def visualize_attention(\n",
    "    model,\n",
    "    atten_model_name,\n",
    "    image_folder=None,\n",
    "    image_idxes=None,\n",
    "    layer_indices=None,\n",
    "    label_names=None,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    show_only_overlay=False\n",
    "):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    target_layers = [model.encoder.layers[i].ln_1 for i in layer_indices]\n",
    "\n",
    "    # Ensure gradients are enabled for target layers\n",
    "    for layer in target_layers:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # Image transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    row_images = []\n",
    "\n",
    "    image_paths = []\n",
    "    if image_folder:\n",
    "        image_paths = [os.path.join(image_folder, fname) for fname in sorted(os.listdir(image_folder)) if fname.endswith(\".jpg\") or fname.endswith(\".jpeg\") or fname.endswith(\".png\")]\n",
    "        if image_idxes is not None:\n",
    "            image_paths = [image_paths[i] for i in image_idxes]\n",
    "\n",
    "    for idx, image_path in enumerate(image_paths):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        img_tensor = transform(image)\n",
    "        img_np = np.array(image.resize((224, 224))).astype(np.float32) / 255.0\n",
    "\n",
    "        input_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "        input_tensor.requires_grad = True\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_class_idx = model(input_tensor).argmax(dim=1).item()\n",
    "\n",
    "        print(f\"Processing {os.path.basename(image_path)} | Pred = {label_names[pred_class_idx]}\")\n",
    "\n",
    "        cams_this_row = []\n",
    "        last_grayscale_cam = None\n",
    "\n",
    "        for layer in target_layers:\n",
    "            cam = GradCAM(model=model, target_layers=[layer], reshape_transform=reshape_transform)\n",
    "            grayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(pred_class_idx)])[0]\n",
    "            cams_this_row.append(grayscale_cam)\n",
    "            last_grayscale_cam = grayscale_cam\n",
    "\n",
    "        overlay_img = show_cam_on_image(img_np, last_grayscale_cam, use_rgb=True, image_weight=0.6)\n",
    "        cams_this_row.append(overlay_img)\n",
    "        row_images.append((cams_this_row, pred_class_idx))\n",
    "\n",
    "    cols = 2\n",
    "    rows = int(np.ceil(len(row_images) * len(layer_indices) / cols)) + 1\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows), squeeze=False)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    img_counter = 0\n",
    "    for cams_this_row, pred in row_images:\n",
    "        predicted = label_names[pred]\n",
    "        images_to_show = [cams_this_row[-1]] if show_only_overlay else cams_this_row\n",
    "\n",
    "        for img in images_to_show:\n",
    "            if img_counter >= len(axes):\n",
    "                break\n",
    "            ax = axes[img_counter]\n",
    "            if img.ndim == 2:\n",
    "                ax.imshow(img, cmap='jet', interpolation='nearest')\n",
    "            else:\n",
    "                ax.imshow(img, interpolation='nearest')\n",
    "\n",
    "            ax.axis('off')\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_title(f\"Pred: {predicted}\", fontsize=10)\n",
    "            img_counter += 1\n",
    "\n",
    "    for ax in axes[img_counter:]:\n",
    "        fig.delaxes(ax)\n",
    "\n",
    "    os.makedirs(GRAD_CAM_DIR, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{GRAD_CAM_DIR}/{atten_model_name}.png\", dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"Saved outputs to gradcam_outputs/{atten_model_name}.png\")"
   ],
   "id": "d8441af964f26c8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from config import IMAGE_DIR\n",
    "\n",
    "# Configuration\n",
    "label_names = val_ds.features['label'].names\n",
    "\n",
    "# Using ViT Base 16\n",
    "atten_model_name = \"vit_b_16\"\n",
    "atten_model_path = \"experiments/torch_model_vit_base_16_cls_epoch10.pth\"\n",
    "top_layers = nn.Sequential(\n",
    "    nn.Linear(768, NUM_CLASS)\n",
    "  )\n",
    "model2 = get_custom_vit(custom_head=top_layers, architecture_name=atten_model_name, weights=None)\n",
    "model2.load_state_dict(torch.load(OUTPUT_DIR+\"/\"+atten_model_path))\n",
    "visualize_attention(\n",
    "    model=model2,\n",
    "    atten_model_name=\"vit_b_16\",\n",
    "    image_folder=IMAGE_DIR,\n",
    "    layer_indices=[-1],\n",
    "    label_names=label_names,\n",
    "    show_only_overlay=True\n",
    ")"
   ],
   "id": "9fd6ebf140635a7d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
