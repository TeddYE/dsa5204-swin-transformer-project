{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import SwinForImageClassification, AutoFeatureExtractor\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from data import load_food"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_ds, val_ds = load_food.load_food(image_size=(224, 224), rand_seed=42, n_class=20)\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load Swin Transformer Base Model\n",
    "path = \"microsoft/swin-base-patch4-window7-224\"\n",
    "finetuned_model = SwinForImageClassification.from_pretrained(path)\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(path)\n",
    "\n",
    "# Modify the classification head to adapt to a 20-class task\n",
    "num_classes = 20\n",
    "finetuned_model.classifier = nn.Linear(finetuned_model.config.hidden_size, num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "finetuned_model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss\n",
    "optimizer = optim.AdamW(finetuned_model.parameters(), lr=5e-5)  # AdamW optimizer"
   ],
   "id": "8c7275495cf49af6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Training function\n",
    "def train(model, train_loader, optimizer, criterion, device, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        for batch in loop:\n",
    "            images = batch[\"image\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).logits\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            loop.set_postfix(loss=loss.item(), acc=correct / total)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, val_loader, criterion, device, class_names=None):\n",
    "    model.eval()\n",
    "    total_loss, total = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images = batch[\"image\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "    print(f\"Accuracy: {acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score (Macro): {f1:.4f}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    np.fill_diagonal(cm, 0)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "        vmin=0,\n",
    "        vmax=21\n",
    "    )\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Swin Transformer\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "89f015d6da4ed02f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from config import OUTPUT_DIR, IMAGE_DIR\n",
    "class_names = [\n",
    "    'ramen', 'carrot_cake', 'beef_carpaccio', 'strawberry_shortcake', 'escargots',\n",
    "    'donuts', 'croque_madame', 'cheese_plate', 'caprese_salad', 'sashimi',\n",
    "    'oysters', 'caesar_salad', 'pho', 'hot_and_sour_soup', 'beef_tartare',\n",
    "    'creme_brulee', 'cup_cakes', 'miso_soup', 'pork_chop', 'paella'\n",
    "]\n",
    "\n",
    "train(finetuned_model, train_loader, optimizer, criterion, device, epochs=5)\n",
    "evaluate(finetuned_model, val_loader, criterion, device, class_names=class_names)\n",
    "\n",
    "# Define the save path\n",
    "save_path = os.path.join(OUTPUT_DIR, \"swin_food101_finetuned.pth\")\n",
    "classifier_path = os.path.join(OUTPUT_DIR, \"classifier.pth\")\n",
    "\n",
    "# Save the entire model's state_dict (including the classifier)\n",
    "torch.save(finetuned_model.state_dict(), save_path)\n",
    "torch.save(finetuned_model.classifier.state_dict(), classifier_path)"
   ],
   "id": "3008eb1f6d93da51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import SwinForImageClassification, AutoFeatureExtractor\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n"
   ],
   "id": "34572371ee88da5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "id2label = {\n",
    "    0: \"ramen\",\n",
    "    1: \"carrot_cake\",\n",
    "    2: \"beef_carpaccio\",\n",
    "    3: \"strawberry_shortcake\",\n",
    "    4: \"escargots\",\n",
    "    5: \"donuts\",\n",
    "    6: \"croque_madame\",\n",
    "    7: \"cheese_plate\",\n",
    "    8: \"caprese_salad\",\n",
    "    9: \"sashimi\",\n",
    "    10: \"oysters\",\n",
    "    11: \"caesar_salad\",\n",
    "    12: \"pho\",\n",
    "    13: \"hot_and_sour_soup\",\n",
    "    14: \"beef_tartare\",\n",
    "    15: \"creme_brulee\",\n",
    "    16: \"cup_cakes\",\n",
    "    17: \"miso_soup\",\n",
    "    18: \"pork_chop\",\n",
    "    19: \"paella\"\n",
    "}\n",
    "\n",
    "class HuggingfaceSwinWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def forward(self, x):\n",
    "        return self.model(x).logits\n",
    "\n",
    "## label prediction function\n",
    "def predict_single_image(image_path, model, feature_extractor, device):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = feature_extractor(image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        pred_index = logits.argmax(dim=1).item()\n",
    "\n",
    "    return pred_index, logits\n",
    "\n",
    "def reshape_transform(tensor, height=7, width=7):\n",
    "    \"\"\"\n",
    "    Swin Transformer-specific reshape function.\n",
    "    Converts tensor from shape [B, (H*W), C] to [B, C, H, W].\n",
    "    \"\"\"\n",
    "    # Get batch size and number of channels\n",
    "    batch_size, num_patches, num_channels = tensor.shape\n",
    "\n",
    "    # Compute height and width of the patch grid\n",
    "    height = width = int(num_patches ** 0.5)\n",
    "\n",
    "    # Reshape and permute dimensions to [B, C, H, W]\n",
    "    tensor = tensor.reshape(batch_size, height, width, num_channels)\n",
    "    tensor = tensor.permute(0, 3, 1, 2)\n",
    "\n",
    "    return tensor\n",
    "\n",
    "def visualize_attention(image_path):\n",
    "    # Step 1: Load and normalize the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_resized = image.resize((224, 224))\n",
    "    rgb_img = np.array(image_resized).astype(np.float32) / 255.0\n",
    "\n",
    "    # Step 2: Preprocess the image for the model\n",
    "    inputs = feature_extractor(image, return_tensors=\"pt\").to(device)\n",
    "    input_tensor = inputs[\"pixel_values\"]\n",
    "\n",
    "    # Step 3: Predict the class\n",
    "    pred_index, logits = predict_single_image(image_path, model, feature_extractor, device)\n",
    "    pred_label = id2label[pred_index]\n",
    "\n",
    "    # Step 4: Grad-CAM visualization\n",
    "    # Select the layernorm_after layer of the last block in the last stage\n",
    "    target_layers = [model.swin.encoder.layers[-1].blocks[-1].layernorm_after]\n",
    "\n",
    "    wrapped_model = HuggingfaceSwinWrapper(model)\n",
    "    cam = GradCAM(\n",
    "        model=wrapped_model,\n",
    "        target_layers=target_layers,\n",
    "        reshape_transform=reshape_transform\n",
    "    )\n",
    "\n",
    "    # Define the target class for Grad-CAM\n",
    "    targets = [ClassifierOutputTarget(pred_index)]\n",
    "\n",
    "    # Ensure the input is a 4D tensor\n",
    "    if len(input_tensor.shape) == 3:\n",
    "        input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0]\n",
    "    visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "\n",
    "    # Step 5: Display the results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    ax1.imshow(image)\n",
    "    ax1.set_title(\"Original Image\")\n",
    "    ax1.axis(\"off\")\n",
    "\n",
    "    ax2.imshow(visualization)\n",
    "    ax2.set_title(f\"Attention Map\\nPredicted: {pred_label}\")\n",
    "    ax2.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "## Load Swin base Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SwinForImageClassification.from_pretrained(path, ignore_mismatched_sizes=True).to(device)\n",
    "model.classifier = torch.nn.Linear(model.config.hidden_size, 20).to(device)\n",
    "\n",
    "## Load backbone parameters\n",
    "state_dict = torch.load(save_path, map_location=\"cpu\")\n",
    "state_dict = {k: v for k, v in state_dict.items() if \"classifier\" not in k}\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "## Load classification head\n",
    "model.classifier.load_state_dict(torch.load(classifier_path, map_location=\"cpu\"))\n",
    "model.eval()\n",
    "##Load feature extractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(path)\n",
    "\n",
    "image_paths = [os.path.join(IMAGE_DIR, fname) for fname in sorted(os.listdir(IMAGE_DIR)) if fname.endswith(\".jpg\") or fname.endswith(\".jpeg\") or fname.endswith(\".png\")]\n",
    "\n",
    "for image_path in image_paths:\n",
    "    visualize_attention(image_path)"
   ],
   "id": "26a93eb4a21cd02a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
